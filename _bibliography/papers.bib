---
---
@inproceedings{dhamne-etal-2025-predicting,
    title = {Predicting Prosodic Boundaries for Children{'}s Texts},
    author = {Dhamne, Mansi and Raman, Sneha and Rao, Preeti},
    booktitle = {Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
    month = nov,
    year = {2025},
    address = {Suzhou, China},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2025.emnlp-main.1623/},
    pdf= {2025.emnlp-main.1623.pdf},
    preview={prosody.png},
    doi = {10.18653/v1/2025.emnlp-main.1623},
    pages = {31863--31873},
    selected = {true},
    abstract =  {Reading fluency in any language requires accurate word decoding but also natural prosodic phrasing i.e the grouping of words into rhythmically and syntactically coherent units. This holds for, both, reading aloud and silent reading. While adults pause meaningfully at clause or punctuation boundaries, children aged 8-13 often insert inappropriate pauses due to limited breath control and underdeveloped prosodic awareness. We present a text-based model to predict cognitively appropriate pause locations in children{'}s reading material. Using a curated dataset of 54 leveled English stories annotated for potential pauses, or prosodic boundaries, by 21 fluent speakers, we find that nearly 30{\%} of pauses occur at non-punctuation locations of the text, highlighting the limitations of using only punctuation-based cues. Our model combines lexical, syntactic, and contextual features with a novel breath duration feature that captures syllable load since the last major boundary. This cognitively motivated approach can model both allowed and ``forbidden'' pauses. The proposed framework supports applications such as child-directed TTS and oral reading fluency assessment where the proper grouping of words is considered critical to reading comprehension.}
}

@article{dhamne-2025-robustness,
    title = {Robust and Interpretable Multimodal Fusion in Med-VQA: A Perturbation Benchmark for Clinical Safety},
    author = {Dhamne, Mansi and Gangwani, Vivek and Kurhade, Swapnali},
    booktitle = {Accepted for SECUREAI4H Workshop at AAAI 2026},
    pdf= {aaai_paper.pdf},
    preview={aaai.png},
    selected = {true},
    abstract =  {Ensuring safety and trust in Medical Visual Question Answering (Med-VQA) systems demands robustness to real-world noise, interpretability for clinical users, and computational efficiency for deployment in safety-critical settings. Current vision–language models rely on opaque, resource-intensive fusion schemes whose vulnerability to adversarial perturbations remains largely unexplored. We present a comprehensive study of adversarial robustness and explainable reasoning in Med-VQA through a systematic benchmark of lightweight, interpretable fusion strategies. Across three standard datasets—VQA-RAD, PATH-VQA, and SLAKE—we evaluate models under clinically plausible visual perturbations (noise, blur, camera, digital) and textual perturbations (medical, natural, synthetic). Results show that ResNet152+BERT achieves strong clean-set accuracy (88.4% on PATH-VQA) but loses up to 19% under impulse noise, while ResNet50+BiLSTM demonstrates higher resilience to text perturbations, outperforming on 15 of 24 linguistic corruption types. Interpretability analyses using Grad-CAM and Integrated Gradients reveal consistent attention realignment under corruption, enabling transparent error attribution. These findings establish one of the first empirical baselines for adversarial robustness in Med-VQA and highlight that efficient, shallow fusion can yield interpretable, resilient reasoning with significantly reduced compute and latency—paving the way for safe, deployable medical AI in real-world clinical workflows.}
}